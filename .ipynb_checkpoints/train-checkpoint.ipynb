{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nikepupu/anaconda3/envs/mine/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage import io, transform\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import pickle\n",
    "from transformers import *\n",
    "from sklearn.externals import joblib\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAD120Dataset(Dataset):\n",
    "    def __init__(self, feature_file,  caption_file , transform=None):\n",
    "\n",
    "        \n",
    "        self.caption = joblib.load(caption_file)\n",
    "        self.caption = [ cap.permute(1,0,2) for cap in self.caption ]\n",
    "        self.features = joblib.load(feature_file)\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.caption)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "            \n",
    "        features = self.features[idx]\n",
    "        captions = self.caption[idx]\n",
    "        \n",
    "       \n",
    "        sample = {\"features\": features, \"captions\":captions}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size_image, input_size_word_embedding, \n",
    "                hidden_size, output_size,num_layers=1):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size_image = input_size_image\n",
    "        self.input_size_word_embedding = input_size_word_embedding\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.lstm_image = nn.LSTM(input_size_image, hidden_size, num_layers,  bidirectional=True)\n",
    "        self.lstm_caption = nn.LSTM(input_size_word_embedding, hidden_size, num_layers,  bidirectional=True)\n",
    "      \n",
    "    \n",
    "    def forward(self, image_features, caption_features):\n",
    "        # Set initial states\n",
    "        \n",
    "        img_batch_len = len(image_features)\n",
    "        caption_batch_len = len(caption_features)\n",
    "        \n",
    " \n",
    "       \n",
    "        h0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        \n",
    "        h1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        c1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "        OUT0 = []\n",
    "        OUT1 = []\n",
    "        for i in range(img_batch_len):\n",
    "            #print(image_features[i].shape)\n",
    "            h0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            c0 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            for j in range(image_features[i].shape[0]):\n",
    "                out0, (h0,c0) = self.lstm_image(image_features[i][j].view(1,1,-1), (h0,c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
    "            OUT0.append(out0)\n",
    "                \n",
    "            \n",
    "        for i in range(caption_batch_len):\n",
    "            h1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            c1 = torch.zeros(self.num_layers*2, 1, self.hidden_size).to(device)\n",
    "            for j in range(caption_features[i].shape[0]):\n",
    "                out1, (h1, c1) = self.lstm_caption(caption_features[i][j].view(1,1,-1), (h1, c1))\n",
    "            OUT1.append(out1)\n",
    "        \n",
    "\n",
    "        OUT0 = torch.stack(OUT0)\n",
    "        OUT1 = torch.stack(OUT1)\n",
    "        OUT0 = OUT0.view(OUT0.shape[0],-1)\n",
    "        OUT1 = OUT1.view(OUT1.shape[0],-1)\n",
    "        return OUT0,OUT1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(512*4*4, 768, 256, 10,1).to(device)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, b in enumerate(train_loader):\n",
    "        imgs = b[0]\n",
    "        captions = b[1]\n",
    "        \n",
    "        img_feature, caption_feature  = imgs, [caption.to(device) for caption in captions]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        model(img_feature, caption_feature)\n",
    "        h_image, h_caption = model(img_feature, caption_feature)\n",
    "\n",
    "    \n",
    "    \n",
    "        l = h_image.shape[0]\n",
    "        \n",
    "        loss = 0\n",
    "        S_image = -1*torch.ones((l,1)).cuda()\n",
    "        S_caption = -1*torch.ones((l,1)).cuda()\n",
    "        accuracy = 0\n",
    "        \n",
    "        for i in range(l):\n",
    "            index_image = 0\n",
    "            for j in range(l):\n",
    "                if S_image[i] <  h_image[i].dot(h_caption[j]):\n",
    "                    S_image[i] =  h_image[i].dot(h_caption[j])\n",
    "                    index_image = j\n",
    "                    \n",
    "            if i == index_image:\n",
    "                accuracy += 1.0\n",
    "            loss += max(0,S_image[i]- h_image[i].dot(h_caption[i])+1)\n",
    "        \n",
    "        \n",
    "        for i in range(l):\n",
    "            index_caption = 0\n",
    "            for j in range(l):    \n",
    "                if S_caption[i] <  h_caption[i].dot(h_image[j]):\n",
    "                    S_caption[i] =  h_caption[i].dot(h_image[j])\n",
    "                    index_caption = j\n",
    "                    \n",
    "            if i == index_caption:\n",
    "                accuracy += 1.0\n",
    "            loss += max(0,S_caption[i]-h_image[i].dot(h_caption[i])+1)\n",
    "        \n",
    "        accuracy /= float(2*l)\n",
    "        loss /= float(l)\n",
    "            \n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #if batch_idx % 16 == 0:\n",
    "        \n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(imgs), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "        \n",
    "\n",
    "        return loss.item(), accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader,epoch):\n",
    "    model.eval()\n",
    "    for batch_idx, b in enumerate(test_loader):\n",
    "        imgs = b[0]\n",
    "        captions = b[1]\n",
    "        \n",
    "        img_feature, caption_feature  = imgs, [caption.to(device) for caption in captions]\n",
    "        \n",
    "        model(img_feature, caption_feature)\n",
    "        h_image, h_caption = model(img_feature, caption_feature)\n",
    "        l = h_image.shape[0]\n",
    "        \n",
    "        \n",
    "        S_image = -1*torch.ones((l,1)).cuda()\n",
    "        S_caption = -1*torch.ones((l,1)).cuda()\n",
    "        accuracy = 0\n",
    "        \n",
    "        for i in range(l):\n",
    "            index_image = 0\n",
    "            for j in range(l):\n",
    "                if S_image[i] <  h_image[i].dot(h_caption[j]):\n",
    "                    S_image[i] =  h_image[i].dot(h_caption[j])\n",
    "                    index_image = j\n",
    "                    \n",
    "            if i == index_image:\n",
    "                accuracy += 1.0\n",
    "            \n",
    "        \n",
    "        \n",
    "        for i in range(l):\n",
    "            index_caption = 0\n",
    "            for j in range(l):    \n",
    "                if S_caption[i] <  h_caption[i].dot(h_image[j]):\n",
    "                    S_caption[i] =  h_caption[i].dot(h_image[j])\n",
    "                    index_caption = j\n",
    "                    \n",
    "            if i == index_caption:\n",
    "                accuracy += 1.0\n",
    "           \n",
    "        \n",
    "        accuracy /= float(2*l)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), 0.01,momentum=0.9,weight_decay=1e-4)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "def my_collate(batch):\n",
    "    img = [item[\"features\"] for item in batch]\n",
    "    caption = [item[\"captions\"] for item in batch]\n",
    "    #caption = torch.LongTensor(caption)\n",
    "    return [img, caption]\n",
    "\n",
    "train_loader = DataLoader( CAD120Dataset('features.bin','caption.bin'), batch_size=128, shuffle = True,\n",
    "                          collate_fn = my_collate)\n",
    "test_loader = DataLoader( CAD120Dataset('features_subject3.bin','caption_subject3.bin'), batch_size=128, shuffle = True,\n",
    "                          collate_fn = my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/164 (0%)]\tLoss: 8.085978\n",
      "Train Epoch: 1 [0/164 (0%)]\tLoss: 10.471212\n",
      "Train Epoch: 2 [0/164 (0%)]\tLoss: 10.587696\n",
      "Train Epoch: 3 [0/164 (0%)]\tLoss: 8.652025\n",
      "Train Epoch: 4 [0/164 (0%)]\tLoss: 6.220052\n",
      "Train Epoch: 5 [0/164 (0%)]\tLoss: 4.639176\n",
      "Train Epoch: 6 [0/164 (0%)]\tLoss: 4.771930\n",
      "Train Epoch: 7 [0/164 (0%)]\tLoss: 4.377268\n",
      "Train Epoch: 8 [0/164 (0%)]\tLoss: 3.662331\n",
      "Train Epoch: 9 [0/164 (0%)]\tLoss: 3.933893\n",
      "Train Epoch: 10 [0/164 (0%)]\tLoss: 3.483151\n",
      "Train Epoch: 11 [0/164 (0%)]\tLoss: 3.933923\n",
      "Train Epoch: 12 [0/164 (0%)]\tLoss: 3.350154\n",
      "Train Epoch: 13 [0/164 (0%)]\tLoss: 3.173579\n",
      "Train Epoch: 14 [0/164 (0%)]\tLoss: 3.202273\n",
      "Train Epoch: 15 [0/164 (0%)]\tLoss: 3.415352\n",
      "Train Epoch: 16 [0/164 (0%)]\tLoss: 3.084725\n",
      "Train Epoch: 17 [0/164 (0%)]\tLoss: 2.876230\n",
      "Train Epoch: 18 [0/164 (0%)]\tLoss: 2.686085\n",
      "Train Epoch: 19 [0/164 (0%)]\tLoss: 2.670209\n",
      "Train Epoch: 20 [0/164 (0%)]\tLoss: 2.609627\n",
      "Train Epoch: 21 [0/164 (0%)]\tLoss: 2.557216\n",
      "Train Epoch: 22 [0/164 (0%)]\tLoss: 2.510124\n",
      "Train Epoch: 23 [0/164 (0%)]\tLoss: 2.499200\n",
      "Train Epoch: 24 [0/164 (0%)]\tLoss: 2.474439\n",
      "Train Epoch: 25 [0/164 (0%)]\tLoss: 2.427821\n",
      "Train Epoch: 26 [0/164 (0%)]\tLoss: 2.367311\n",
      "Train Epoch: 27 [0/164 (0%)]\tLoss: 2.324925\n",
      "Train Epoch: 28 [0/164 (0%)]\tLoss: 2.312098\n",
      "Train Epoch: 29 [0/164 (0%)]\tLoss: 2.324764\n",
      "Train Epoch: 30 [0/164 (0%)]\tLoss: 2.296088\n",
      "Train Epoch: 31 [0/164 (0%)]\tLoss: 2.296358\n",
      "Train Epoch: 32 [0/164 (0%)]\tLoss: 2.286327\n",
      "Train Epoch: 33 [0/164 (0%)]\tLoss: 2.282112\n",
      "Train Epoch: 34 [0/164 (0%)]\tLoss: 2.229417\n",
      "Train Epoch: 35 [0/164 (0%)]\tLoss: 2.280978\n",
      "Train Epoch: 36 [0/164 (0%)]\tLoss: 2.232924\n",
      "Train Epoch: 37 [0/164 (0%)]\tLoss: 2.229470\n",
      "Train Epoch: 38 [0/164 (0%)]\tLoss: 2.227000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1000):\n",
    "    loss,accuracy = train(model, 'cuda:0', train_loader, optimizer, epoch)\n",
    "    writer.add_scalars('data/loss', {'loss': loss}, epoch)\n",
    "    writer.add_scalars('data/train_accuracy', {'train_accuracy': accuracy}, epoch)\n",
    "    accuracy = test(model, 'cuda:0', test_loader,epoch)\n",
    "    writer.add_scalars('data/test_accuracy', {'test_accuracy': accuracy}, epoch)\n",
    "    if epoch == 50:\n",
    "        optimizer = optim.SGD(model.parameters(), 0.01,momentum=0.9,weight_decay=1e-4)\n",
    "writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "mine"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
